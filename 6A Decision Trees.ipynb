{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision Tree is one of most popularly used machine learning algorithm.\n",
    "# It is mostly used for classification problems. But it can also be used for regression problems as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#references: \n",
    "#https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a decision tree: Loan application\n",
    "![title](img/dt1.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let’s just take a famous dataset in the machine learning world which is weather dataset(playing game Y or N based on weather condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt2.jpeg) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have four X values (outlook,temp,humidity and windy) being categorical and one y value (play Y or N) also being categorical.\n",
    "\n",
    "so we need to learn the mapping (what machine learning always does) between X and y.\n",
    "\n",
    "This is a binary classification problem, lets build the tree\n",
    "\n",
    "To create a tree, we need to have a root node first and we know that nodes are features/attributes(outlook,temp,humidity and windy),"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "so which one do we need to pick first??\n",
    "Answer: determine the attribute that best classifies the training data; use this attribute at the root of the tree. Repeat this process at for each branch.\n",
    "\n",
    "This means we are performing top-down, greedy search through the space of possible decision trees.\n",
    "\n",
    "okay so how do we choose the best attribute?\n",
    "Answer: use the attribute with the highest information gain\n",
    "\n",
    "In order to define information gain precisely, we begin by defining a measure commonly used in information theory, called entropy that characterizes the (im)purity of an arbitrary collection of examples.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt3.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For a binary classification problem\n",
    "\n",
    "If all examples are positive or all are negative then entropy will be zero i.e, low.\n",
    "If half of the examples are of positive class and half are of negative class then entropy is one i.e, high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt4.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Okay lets apply these metrics to our dataset to split the data(getting the root node)\n",
    "\n",
    "Steps:\n",
    "    1.compute the entropy for data-set\n",
    "2.for every attribute/feature:\n",
    "       1.calculate entropy for all categorical values\n",
    "       2.take average information entropy for the current attribute\n",
    "       3.calculate gain for the current attribute\n",
    "3. pick the highest gain attribute.\n",
    "4. Repeat until we get the tree we desired."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compute the entropy for the weather data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt5.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For every feature calculate the entropy and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt6.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similarity we can calculate for other two attributes(Humidity and Temp).\n",
    "\n",
    "Pick the highest gain attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt7.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So our root node is Outlook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt8.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Repeat the same thing for sub-trees till we get the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt9.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally we get the tree something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dt10.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
